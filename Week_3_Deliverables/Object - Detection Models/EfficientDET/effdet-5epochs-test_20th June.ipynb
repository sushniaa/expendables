{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3346317,"sourceType":"datasetVersion","datasetId":1935874},{"sourceId":8748321,"sourceType":"datasetVersion","datasetId":5253896}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q effdet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T12:37:40.228177Z","iopub.execute_input":"2025-06-20T12:37:40.229025Z","iopub.status.idle":"2025-06-20T12:38:51.216013Z","shell.execute_reply.started":"2025-06-20T12:37:40.228992Z","shell.execute_reply":"2025-06-20T12:38:51.215139Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q torch torchvision albumentations pycocotools opencv-python pytorch-lightning timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T12:38:51.217618Z","iopub.execute_input":"2025-06-20T12:38:51.217897Z","iopub.status.idle":"2025-06-20T12:38:54.716689Z","shell.execute_reply.started":"2025-06-20T12:38:51.217873Z","shell.execute_reply":"2025-06-20T12:38:54.715712Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport cv2\nimport xml.etree.ElementTree as ET\nfrom pathlib import Path\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport pytorch_lightning as pl\nfrom effdet.anchors import Anchors, AnchorLabeler\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\nfrom effdet.efficientdet import HeadNet\nfrom effdet import create_model\nimport timm\nimport random\nimport torchmetrics\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T14:01:55.691789Z","iopub.execute_input":"2025-06-20T14:01:55.692201Z","iopub.status.idle":"2025-06-20T14:01:55.700382Z","shell.execute_reply.started":"2025-06-20T14:01:55.692171Z","shell.execute_reply":"2025-06-20T14:01:55.699536Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Set device and random seed\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Define dataset paths\nDATA_DIR = \"/kaggle/input/ip102-dataset\"\nIMG_DIR = os.path.join(DATA_DIR, \"JPEGImages\")\nANN_DIR = os.path.join(DATA_DIR, \"Annotations\")\nTRAIN_LIST = os.path.join(\"/kaggle/working\", \"train.txt\")\nVAL_LIST = os.path.join(\"/kaggle/working\", \"val.txt\")\nTEST_LIST = os.path.join(DATA_DIR, \"test.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T12:39:10.380144Z","iopub.execute_input":"2025-06-20T12:39:10.381451Z","iopub.status.idle":"2025-06-20T12:39:10.390936Z","shell.execute_reply.started":"2025-06-20T12:39:10.381425Z","shell.execute_reply":"2025-06-20T12:39:10.390517Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Split trainval into train and val (if not already done)\ndef split_trainval(trainval_path, train_out_path, val_out_path, train_ratio=0.9):\n    if not os.path.exists(train_out_path) or not os.path.exists(val_out_path):\n        with open(trainval_path, 'r') as f:\n            img_ids = [line.strip() for line in f.readlines()]\n        random.shuffle(img_ids)\n        split_idx = int(len(img_ids) * train_ratio)\n        train_ids = img_ids[:split_idx]\n        val_ids = img_ids[split_idx:]\n        with open(train_out_path, 'w') as f:\n            f.write('\\n'.join(train_ids))\n        with open(val_out_path, 'w') as f:\n            f.write('\\n'.join(val_ids))\n        print(f\"Train set: {len(train_ids)} images, Val set: {len(val_ids)} images\")\n\n# Execute split if files don't exist\nsplit_trainval(os.path.join(DATA_DIR, \"trainval.txt\"), TRAIN_LIST, VAL_LIST)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T12:39:10.391430Z","iopub.execute_input":"2025-06-20T12:39:10.391685Z","iopub.status.idle":"2025-06-20T12:39:10.433380Z","shell.execute_reply.started":"2025-06-20T12:39:10.391663Z","shell.execute_reply":"2025-06-20T12:39:10.432789Z"}},"outputs":[{"name":"stdout","text":"Train set: 13660 images, Val set: 1518 images\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Custom Dataset for IP102\nimport xml.etree.ElementTree as ET\nfrom xml.etree.ElementTree import ParseError\nclass IP102Dataset(Dataset):\n    def __init__(self, img_dir, ann_dir, img_list, transform=None):\n        self.img_dir = Path(img_dir)\n        self.ann_dir = Path(ann_dir)\n        with open(img_list, 'r') as f:\n            self.img_ids = [line.strip() for line in f.readlines()]\n        self.transform = transform\n        self.class_map = {i: str(i) for i in range(102)}  # 102 classes\n\n    def __len__(self):\n        return len(self.img_ids)\n\n\n    def __getitem__(self, idx):\n        try:\n            img_id = self.img_ids[idx]\n            img_path = self.img_dir / f\"{img_id}.jpg\"\n            ann_path = self.ann_dir / f\"{img_id}.xml\"\n    \n            # Load image\n            img = cv2.imread(str(img_path))\n            if img is None:\n                raise FileNotFoundError(f\"Could not read image: {img_path}\")\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            original_size = img.shape[:2]  # (H, W)\n    \n            # Parse XML annotation\n            tree = ET.parse(str(ann_path))\n            root = tree.getroot()\n            boxes = []\n            labels = []\n    \n            for obj in root.findall('object'):\n                label = int(obj.find('name').text)  # Class ID (0â€“101)\n                bbox = obj.find('bndbox')\n                xmin = float(bbox.find('xmin').text)\n                ymin = float(bbox.find('ymin').text)\n                xmax = float(bbox.find('xmax').text)\n                ymax = float(bbox.find('ymax').text)\n    \n                # Filter invalid boxes\n                if xmax > xmin and ymax > ymin:\n                    boxes.append([xmin, ymin, xmax, ymax])\n                    labels.append(label)\n    \n            boxes = np.array(boxes, dtype=np.float32)\n            labels = np.array(labels, dtype=np.int64)\n    \n            # If no valid boxes, insert dummy box\n            if len(boxes) == 0:\n                boxes = np.array([[0, 0, 1, 1]], dtype=np.float32)\n                labels = np.array([0], dtype=np.int64)\n    \n            # Apply Albumentations transform\n            if self.transform:\n                transformed = self.transform(image=img, bboxes=boxes, class_labels=labels)\n                img = transformed[\"image\"]\n                boxes = np.array(transformed[\"bboxes\"], dtype=np.float32)\n                labels = np.array(transformed[\"class_labels\"], dtype=np.int64)\n    \n                transformed_height, transformed_width = img.shape[1:3]\n                scale = min(transformed_height / original_size[0], transformed_width / original_size[1])\n            else:\n                scale = 1.0\n    \n            # Fallback again if transform removed all boxes\n            if len(boxes) == 0:\n                boxes = np.array([[0, 0, 1, 1]], dtype=np.float32)\n                labels = np.array([0], dtype=np.int64)\n    \n            target = {\n                \"bbox\": torch.tensor(boxes, dtype=torch.float32),\n                \"cls\": torch.tensor(labels, dtype=torch.int64),\n                \"image_id\": torch.tensor([idx], dtype=torch.int64),\n                \"img_scale\": torch.tensor([scale], dtype=torch.float32),\n                \"img_size\": torch.tensor([original_size], dtype=torch.float32)\n            }\n    \n            return img, target\n    \n        except (ParseError, FileNotFoundError, cv2.error) as e:\n            print(f\"Skipping sample at index {idx} due to error: {e}\")\n            return None\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:03:55.440836Z","iopub.execute_input":"2025-06-20T13:03:55.441146Z","iopub.status.idle":"2025-06-20T13:03:55.455461Z","shell.execute_reply.started":"2025-06-20T13:03:55.441124Z","shell.execute_reply":"2025-06-20T13:03:55.454774Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Define Data Augmentation\ntransform = A.Compose([\n    A.Resize(512, 512),  # Fixed size to ensure uniform dimensions\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n\n# Create Datasets and DataLoaders\ntrain_dataset = IP102Dataset(IMG_DIR, ANN_DIR, TRAIN_LIST, transform=transform)\nval_dataset = IP102Dataset(IMG_DIR, ANN_DIR, VAL_LIST, transform=transform)\ntest_dataset = IP102Dataset(IMG_DIR, ANN_DIR, TEST_LIST, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:03:58.735294Z","iopub.execute_input":"2025-06-20T13:03:58.735561Z","iopub.status.idle":"2025-06-20T13:03:58.752498Z","shell.execute_reply.started":"2025-06-20T13:03:58.735541Z","shell.execute_reply":"2025-06-20T13:03:58.751857Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def collate_fn(batch):\n    # Remove any failed samples\n    batch = [item for item in batch if item is not None]\n    \n    if len(batch) == 0:\n        return None  # let Dataloader retry\n\n    images, targets = zip(*batch)\n    images = torch.stack(images)\n\n    max_boxes = max([len(t[\"bbox\"]) for t in targets])\n    \n    padded_bboxes = torch.stack([\n        torch.cat([t[\"bbox\"], torch.zeros(max_boxes - t[\"bbox\"].size(0), 4, dtype=torch.float32)], dim=0)\n        for t in targets\n    ])\n    padded_labels = torch.stack([\n        torch.cat([t[\"cls\"], torch.full((max_boxes - t[\"cls\"].size(0),), -1, dtype=torch.int64)], dim=0)\n        for t in targets\n    ])\n\n    adjusted_targets = {\n        \"bbox\": padded_bboxes,\n        \"cls\": padded_labels,\n        \"image_id\": torch.stack([t[\"image_id\"] for t in targets]),\n        \"img_scale\": torch.cat([t[\"img_scale\"] for t in targets], dim=0),\n        \"img_size\": torch.cat([t[\"img_size\"] for t in targets], dim=0)\n    }\n\n    return images, adjusted_targets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:11:49.571295Z","iopub.execute_input":"2025-06-20T13:11:49.571620Z","iopub.status.idle":"2025-06-20T13:11:49.579720Z","shell.execute_reply.started":"2025-06-20T13:11:49.571588Z","shell.execute_reply":"2025-06-20T13:11:49.578944Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Create Datasets and DataLoaders\ntrain_dataset = IP102Dataset(IMG_DIR, ANN_DIR, TRAIN_LIST, transform=transform)\nval_dataset = IP102Dataset(IMG_DIR, ANN_DIR, VAL_LIST, transform=transform)\ntest_dataset = IP102Dataset(IMG_DIR, ANN_DIR, TEST_LIST, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:11:55.106682Z","iopub.execute_input":"2025-06-20T13:11:55.106979Z","iopub.status.idle":"2025-06-20T13:11:55.118139Z","shell.execute_reply.started":"2025-06-20T13:11:55.106947Z","shell.execute_reply":"2025-06-20T13:11:55.117484Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import torch\nimport pytorch_lightning as pl\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\n\nclass EfficientDetLightning(pl.LightningModule):\n    def __init__(self, num_classes=102):\n        super().__init__()\n        config = get_efficientdet_config('tf_efficientdet_d0')\n        config.num_classes = num_classes\n        config.image_size = (512, 512)\n\n        # Build base model\n        base_model = create_model(\n            model_name='tf_efficientdet_d0',\n            num_classes=num_classes,\n            pretrained=False\n        )\n        base_model.class_net = HeadNet(config, num_outputs=num_classes)\n        self.model = DetBenchTrain(base_model, config)\n\n        # Metric tracker\n        self.map_metric = MeanAveragePrecision(iou_type=\"bbox\", class_metrics=True)\n        self.save_hyperparameters()\n\n    def forward(self, images, targets=None):\n        return self.model(images, targets)\n\n    def training_step(self, batch, batch_idx):\n        images, targets = batch\n        images = images.to(self.device)\n    \n        # Move all fields in the batched target dict to device\n        targets = {k: v.to(self.device) for k, v in targets.items()}\n    \n        # Pass directly to the model (DetBenchTrain supports this format)\n        loss_dict = self.model(images, targets)\n        loss = loss_dict['loss']\n    \n        # Log training loss\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n    def validation_step(self, batch, batch_idx):\n        images, targets = batch\n        images = images.to(self.device)\n        targets = {k: v.to(self.device) for k, v in targets.items()}\n    \n        # Compute and log loss\n        loss_dict = self.model(images, targets)\n        self.log(\"val_loss\", loss_dict['loss'], on_epoch=True, prog_bar=True)\n    \n        self.model.eval()\n        with torch.no_grad():\n            detections = self.model.model(images)  # inference mode: returns list of dicts\n    \n        self.model.train()\n    \n        # Build a lookup from image_id -> {boxes, labels}\n        image_id_to_target = {}\n        for i in range(images.size(0)):\n            valid_mask = targets[\"cls\"][i] != -1\n            if valid_mask.sum() == 0:\n                continue  # no valid GT\n            img_id = targets[\"image_id\"][i].item()\n            image_id_to_target[img_id] = {\n                \"boxes\": targets[\"bbox\"][i][valid_mask],\n                \"labels\": targets[\"cls\"][i][valid_mask]\n            }\n    \n        # Build aligned target list for each prediction\n        aligned_targets = []\n        aligned_preds = []\n    \n        for pred in detections:\n            img_id = pred[\"image_id\"].item() if \"image_id\" in pred else None\n            if img_id is not None and img_id in image_id_to_target:\n                aligned_preds.append({\n                    \"boxes\": pred[\"boxes\"].to(self.device),\n                    \"scores\": pred[\"scores\"].to(self.device),\n                    \"labels\": pred[\"labels\"].to(self.device)\n                })\n                aligned_targets.append(image_id_to_target[img_id])\n    \n        # Final check before update\n        if len(aligned_preds) != len(aligned_targets):\n            print(\"Some predictions skipped due to unmatched image_id.\")\n            return  # skip this batch safely\n\n        if aligned_preds and aligned_targets:\n            print(f\"\\n[DEBUG] Sample Prediction (epoch {self.current_epoch}, batch {batch_idx}):\")\n            print(aligned_preds[0])\n            print(\"[DEBUG] Sample Target:\")\n            print(aligned_targets[0])\n        \n        if len(aligned_preds) == len(aligned_targets) and len(aligned_preds) > 0:\n            self.map_metric.update(aligned_preds, aligned_targets)\n    \n        #self.map_metric.update(aligned_preds, aligned_targets)\n\n\n\n    def on_validation_epoch_end(self):\n        results = self.map_metric.compute()\n        map_50 = results[\"map_50\"]\n        map_95 = results[\"map\"]\n        self.log(\"mAP@50\", map_50, prog_bar=True)\n        self.log(\"mAP@95\", map_95, prog_bar=True)\n        print(f\"\\nEpoch {self.current_epoch+1} - mAP@50: {map_50:.4f}, mAP@95: {map_95:.4f}\")\n        self.map_metric.reset()\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=75)\n        return [optimizer], [scheduler]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T14:04:01.539211Z","iopub.execute_input":"2025-06-20T14:04:01.539859Z","iopub.status.idle":"2025-06-20T14:04:01.555585Z","shell.execute_reply.started":"2025-06-20T14:04:01.539833Z","shell.execute_reply":"2025-06-20T14:04:01.554857Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:58:53.995940Z","iopub.execute_input":"2025-06-20T13:58:53.996650Z","iopub.status.idle":"2025-06-20T13:58:54.018415Z","shell.execute_reply.started":"2025-06-20T13:58:53.996623Z","shell.execute_reply":"2025-06-20T13:58:54.017497Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3191715017.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n[DEBUG] Sample Prediction:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdetections\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[DEBUG] Sample Target:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtarget_list\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'detections' is not defined"],"ename":"NameError","evalue":"name 'detections' is not defined","output_type":"error"}],"execution_count":49},{"cell_type":"code","source":"from pytorch_lightning.callbacks import TQDMProgressBar\n\nclass PersistentProgressBar(TQDMProgressBar):\n    def init_train_tqdm(self):\n        bar = super().init_train_tqdm()\n        bar.leave = True  # Do not erase bar after epoch ends\n        return bar\n\n    def init_validation_tqdm(self):\n        bar = super().init_validation_tqdm()\n        bar.leave = True\n        return bar\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:31:50.665032Z","iopub.execute_input":"2025-06-20T13:31:50.665354Z","iopub.status.idle":"2025-06-20T13:31:50.669712Z","shell.execute_reply.started":"2025-06-20T13:31:50.665334Z","shell.execute_reply":"2025-06-20T13:31:50.668975Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"model = EfficientDetLightning(num_classes=102)\n\ntrainer = pl.Trainer(\n    max_epochs=20,\n    accelerator=\"auto\",\n    devices=\"auto\",\n    precision=\"32-true\",\n    log_every_n_steps=100,\n    val_check_interval=1.0,\n    callbacks=[PersistentProgressBar(refresh_rate=20)]\n)\n\ntrainer.fit(model, train_loader, val_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T14:04:04.538661Z","iopub.execute_input":"2025-06-20T14:04:04.538943Z","iopub.status.idle":"2025-06-20T16:46:20.705434Z","shell.execute_reply.started":"2025-06-20T14:04:04.538922Z","shell.execute_reply":"2025-06-20T16:46:20.704764Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 1 - mAP@50: -1.0000, mAP@95: -1.0000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MeanAveragePrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n  warnings.warn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59e0450b26a740bea027582c1a734ae4"}},"metadata":{}},{"name":"stdout","text":"Skipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19f72486057f4354aecde335baf3775d"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 1 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c37b0b5751422e96be4c1aee34b4af"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 2 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc561ff8ca174d2a9f7391ee62706f00"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 3 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a7999cac22249f89d1445a6692233b8"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 4 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c1f183d90d745cc9031fa5376c4f71c"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 5 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7c658d46cc4497f86de55a9e767b367"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 6 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54aa6133ebb14ed6bea59363a6d9c4ba"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 7 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"426434c8710b43b5b2241ebd4e7eee81"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 8 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27f454cf896f44b2987b0fdbe8564796"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 9 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b5c107f2bdd43f48d929102949411e4"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 10 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad7a9e2857e2458c9a4ea8f73433fd80"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 11 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a94f287b24048b999d9bce7d51513b3"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 12 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5cb9ceed2304b7c8ea959ac63e420d5"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 13 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5e1091e755c4d088d6a9fcdb4cbad2e"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 14 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45e1e11c92554a139e9a397668b6cff1"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 15 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f32f7c2ff80c494cbc9c283e481c36fb"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 16 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"864bc88b721645de94d17cdd582b5582"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 17 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ff1de526174ff3bfe901daa0731836"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 18 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e76c1497d204494b90ed315e2a65b13d"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 19 - mAP@50: -1.0000, mAP@95: -1.0000\nSkipping sample at index 4615 due to error: junk after document element: line 27, column 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4149005a24d454e8c67342d22340421"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 20 - mAP@50: -1.0000, mAP@95: -1.0000\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"import torch\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nimport numpy as np\nfrom effdet.bench import DetBenchPredict\n\n\n\ndef evaluate_model(model, test_loader, device):\n    model.eval()\n    test_losses = []\n    map_metric = MeanAveragePrecision(iou_type=\"bbox\")\n\n    # Use EfficientDet backbone and wrap with DetBenchPredict for inference\n    backbone_model = model.model.model  # raw EfficientDet\n    inference_model = DetBenchPredict(backbone_model).to(device)\n    inference_model.eval()\n\n    with torch.no_grad():\n        for batch_idx, (images, targets) in enumerate(test_loader):\n            images = images.to(device)\n            targets = {k: v.to(device) for k, v in targets.items()}\n\n            # Compute loss using training model\n            outputs = model(images, targets)\n            loss = outputs['loss']\n            test_losses.append(loss.item())\n\n            # Get predictions using inference wrapper\n            detections = inference_model(images)\n\n            target_list = []\n            pred_list = []\n\n            for i in range(len(images)):\n                # Filter out padding (-1)\n                valid_mask = targets[\"cls\"][i] != -1\n                if valid_mask.sum() == 0:\n                    continue\n\n                target_list.append({\n                    \"boxes\": targets[\"bbox\"][i][valid_mask],\n                    \"labels\": targets[\"cls\"][i][valid_mask]\n                })\n\n                pred = detections[i]\n\n                # CASE 1: EfficientDet returns dict (newer versions)\n                if isinstance(pred, dict):\n                    if all(k in pred for k in [\"boxes\", \"scores\", \"labels\"]) and pred[\"boxes\"].numel() > 0:\n                        pred_list.append({\n                            \"boxes\": pred[\"boxes\"],\n                            \"scores\": pred[\"scores\"],\n                            \"labels\": pred[\"labels\"]\n                        })\n\n                # CASE 2: EfficientDet returns Nx6 tensor [x1, y1, x2, y2, score, label]\n                elif isinstance(pred, torch.Tensor) and pred.ndim == 2 and pred.shape[1] == 6:\n                    boxes = pred[:, :4]\n                    scores = pred[:, 4]\n                    labels = pred[:, 5].long()\n                    if boxes.numel() > 0:\n                        pred_list.append({\n                            \"boxes\": boxes,\n                            \"scores\": scores,\n                            \"labels\": labels\n                        })\n\n                else:\n                    print(f\"[WARN] Skipping unexpected prediction format for sample {i}: {type(pred)}\")\n\n            if len(pred_list) == len(target_list) and len(pred_list) > 0:\n                map_metric.update(pred_list, target_list)\n\n    results = map_metric.compute()\n    avg_loss = np.mean(test_losses)\n\n    print(f\"\\nðŸ“Š Evaluation Summary:\")\n    print(f\"Average Test Loss: {avg_loss:.4f}\")\n    print(f\"mAP@50: {results['map_50']:.4f}\")\n    print(f\"mAP@95: {results['map']:.4f}\")\n\n    return avg_loss, results\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\navg_loss, test_map_results = evaluate_model(model, test_loader, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:59:42.545811Z","iopub.execute_input":"2025-06-20T16:59:42.546576Z","iopub.status.idle":"2025-06-20T17:02:53.233792Z","shell.execute_reply.started":"2025-06-20T16:59:42.546548Z","shell.execute_reply":"2025-06-20T17:02:53.233027Z"}},"outputs":[{"name":"stdout","text":"\nðŸ“Š Evaluation Summary:\nAverage Test Loss: nan\nmAP@50: 0.0388\nmAP@95: 0.0388\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"# Evaluate on Test Set\nmodel.eval()\ntest_losses = []\nwith torch.no_grad():\n    for images, targets in test_loader:\n        images = images.to(device)\n        # Move all tensors in targets to device\n        adjusted_targets = {k: v.to(device) for k, v in targets.items()}\n        outputs = model(images, adjusted_targets)\n        loss = outputs['loss']\n        test_losses.append(loss.item())\nprint(f\"Average Test Loss: {np.mean(test_losses):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:53:54.130997Z","iopub.execute_input":"2025-06-20T16:53:54.131748Z","iopub.status.idle":"2025-06-20T16:53:54.725563Z","shell.execute_reply.started":"2025-06-20T16:53:54.131726Z","shell.execute_reply":"2025-06-20T16:53:54.724166Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2752146618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Move all tensors in targets to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0madjusted_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtest_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1317372254.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/effdet/bench.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, target)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mclass_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_labeler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# target should contain pre-computed anchor labels if labeler not present in bench\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/effdet/efficientdet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mx_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_hooks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/layers/conv2d_same.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         return conv2d_same(\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/layers/conv2d_same.py\u001b[0m in \u001b[0;36mconv2d_same\u001b[0;34m(x, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     25\u001b[0m ):\n\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_same\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"],"ename":"RuntimeError","evalue":"Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same","output_type":"error"}],"execution_count":57},{"cell_type":"code","source":"# Export Model (Optional)\ntorch.save(model.state_dict(), \"efficientdet_ip102.pth\")\nprint(\"Model weights saved to efficientdet_ip102.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:53:34.705230Z","iopub.execute_input":"2025-06-20T16:53:34.705819Z","iopub.status.idle":"2025-06-20T16:53:34.780388Z","shell.execute_reply.started":"2025-06-20T16:53:34.705790Z","shell.execute_reply":"2025-06-20T16:53:34.779522Z"}},"outputs":[{"name":"stdout","text":"Model weights saved to efficientdet_ip102.pth\n","output_type":"stream"}],"execution_count":56}]}